import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import jieba
import jieba.analyse
from wordcloud import WordCloud
from snownlp import SnowNLP
import warnings
from datetime import datetime
import os
import yaml
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class BilibiliTextAnalyzer:
    def __init__(self, config_path="config.yaml"):
        self.comments_data = []
        self.contents_data = []
        self.creators_data = []
        self.config = self._load_config(config_path)
  
        # ç²¾ç®€åœç”¨è¯åˆ—è¡¨ï¼Œä¿ç•™æ›´å¤šæœ‰æ„ä¹‰è¯æ±‡
        self.stop_words = self._load_stop_words()

        # æ·»åŠ è‡ªå®šä¹‰è¯å…¸
        self._add_custom_words()

    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ {config_path}ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")
            return {}

    def _load_stop_words(self):
        """åŠ è½½åœç”¨è¯åˆ—è¡¨ - ç²¾ç®€ç‰ˆæœ¬"""
        # åªä¿ç•™æœ€æ ¸å¿ƒçš„åœç”¨è¯ï¼Œå‡å°‘è¿‡åº¦è¿‡æ»¤
        basic_stop_words = {
            # æœ€åŸºç¡€çš„æ— æ„ä¹‰è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰',
            'è¿˜', 'è¿™', 'é‚£', 'æŠŠ', 'è¢«', 'ä»', 'ä¸', 'åŠ', 'ä»¥', 'ä¸º', 'è€Œ', 'æˆ–',
            'ä½†', 'å¯', 'èƒ½', 'å°†', 'å·²', 'æ‰€', 'ä¹‹', 'å…¶', 'ç­‰', 'å¦‚', 'æ¯”',
            'å†', 'è¿˜æ˜¯', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ',
            
            # Bç«™å¹³å°ç›¸å…³æ— æ„ä¹‰è¯ï¼ˆå‡å°‘ï¼‰
            'å›å¤', 'è¯„è®º', 'å…³æ³¨', 'ç‚¹èµ', 'æ”¶è—', 'åˆ†äº«', 'å¼¹å¹•', 'æŠ•å¸',
            'ä¸‰è¿', 'ä¸€é”®ä¸‰è¿', 'up', 'UP', 'upä¸»', 'UPä¸»',
            
            # æ˜æ˜¾çš„æ— æ„ä¹‰è¡¨è¾¾
            'å“ˆå“ˆ', 'å“ˆå“ˆå“ˆ', 'å˜»å˜»', 'å‘µå‘µ', 'å—¯', 'å•Š', 'å“¦', 'é¢',
            'doge', 'hhh', 'hhhh', 'emmm', '6666', '666',
            
            # è¿‡äºé€šç”¨çš„è¯æ±‡
            'ä¸œè¥¿', 'åœ°æ–¹', 'æ—¶é—´', 'ç©ºé—´', 'æœºä¼š', 'å¯èƒ½æ€§',
            'å¥½çš„', 'ä¸é”™', 'æŒºå¥½', 'å¾ˆå¥½', 'å¤ªå¥½äº†', 'è§‰å¾—', 
            'å¯èƒ½', 'è¿˜æœ‰', 'éœ€è¦', 'æ—¶å€™', 'äº‹æƒ…', 'é—®é¢˜', 
            'æ–¹æ³•', 'ä¸ä¼š', 'åº”è¯¥', 'å¯ä»¥', 'æƒ³è¦', 'çŸ¥é“', 
            'äº†è§£', 'åªèƒ½', 'å¸Œæœ›', 'ç›¸ä¿¡', 'æœŸå¾…', 'å–œæ¬¢', 
            'çˆ±', 'è§†é¢‘', 'å†…å®¹', 'åˆ›ä½œ', 'ä½œå“', 'åˆ†äº«',
            'ä¸èƒ½', 'ä¸è¡Œ', 'ä¸å¯ä»¥', 'ä¸æƒ³', 'ä¸å–œæ¬¢',
        }
        
        return basic_stop_words
    
    def _add_custom_words(self):
        """æ·»åŠ è‡ªå®šä¹‰è¯å…¸"""
        # ç»æµé‡‘èç›¸å…³ä¸“ä¸šè¯æ±‡
        economic_words = [
            'å®è§‚ç»æµ', 'å¾®è§‚ç»æµ', 'è´§å¸æ”¿ç­–', 'è´¢æ”¿æ”¿ç­–', 'é€šèƒ€', 'é€šç¼©', 'GDP',
            'æ¶ˆè´¹å‡çº§', 'æ¶ˆè´¹é™çº§', 'äº§ä¸šå‡çº§', 'ä¾›ç»™ä¾§æ”¹é©', 'éœ€æ±‚ä¾§ç®¡ç†',
            'èµ„æœ¬å¸‚åœº', 'è‚¡ç¥¨å¸‚åœº', 'å€ºåˆ¸å¸‚åœº', 'å¤–æ±‡å¸‚åœº', 'æœŸè´§å¸‚åœº',
            'åˆ©ç‡', 'æ±‡ç‡', 'é€šèƒ€ç‡', 'å¤±ä¸šç‡', 'å¢é•¿ç‡', 'ç»æµå±æœº', 'é‡‘èå±æœº',
            'æˆ¿åœ°äº§', 'è‚¡ç¥¨', 'åŸºé‡‘', 'å€ºåˆ¸', 'æœŸè´§', 'å¤–æ±‡', 'æŠ•èµ„', 'ç†è´¢',
            'å¤®è¡Œ', 'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'ä¿¡è´·', 'è´·æ¬¾', 'å­˜æ¬¾', 'å‚¨è“„'
        ]
        
        # Bç«™ç›¸å…³ä¸“ä¸šè¯æ±‡
        bilibili_words = [
            'å“”å“©å“”å“©', 'bilibili', 'Bç«™', 'é¬¼ç•œ', 'ç•ªå‰§', 'çºªå½•ç‰‡', 'ç”Ÿæ´»åŒº',
            'ç§‘æŠ€åŒº', 'æ¸¸æˆåŒº', 'éŸ³ä¹åŒº', 'èˆè¹ˆåŒº', 'å½±è§†åŒº', 'çŸ¥è¯†åŒº'
        ]
        
        # ç¤¾ä¼šç»æµçƒ­è¯
        social_economic_words = [
            'å†…å·', 'èººå¹³', '996', '007', 'æ‰“å·¥äºº', 'ç¤¾ç•œ', 'ä½›ç³»', 'æ‘†çƒ‚',
            'æ¶ˆè´¹ä¸»ä¹‰', 'æç®€ä¸»ä¹‰', 'æ–­èˆç¦»', 'ç²¾ç¥å†…è€—', 'ç¤¾ä¼šè¾¾å°”æ–‡',
            'é˜¶çº§å›ºåŒ–', 'ç¤¾ä¼šæµåŠ¨', 'æ•™è‚²å†…å·', 'å°±ä¸šå‹åŠ›', 'ç”Ÿè‚²ç‡',
            'è€é¾„åŒ–', 'å°‘å­åŒ–', 'äººå£çº¢åˆ©', 'äº§ä¸šè½¬å‹', 'æ•°å­—ç»æµ'
        ]
        
        # æ·»åŠ åˆ°jiebaè¯å…¸
        for word in economic_words + bilibili_words + social_economic_words:
            jieba.add_word(word)
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            with open('data/search_comments_2025-07-14.json', 'r', encoding='utf-8') as f:
                self.comments_data = json.load(f)
            with open('data/search_contents_2025-07-14.json', 'r', encoding='utf-8') as f:
                self.contents_data = json.load(f)
            with open('data/search_creators_2025-07-14.json', 'r', encoding='utf-8') as f:
                self.creators_data = json.load(f)
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"è¯„è®ºæ•°æ®: {len(self.comments_data)} æ¡")
            print(f"è§†é¢‘æ•°æ®: {len(self.contents_data)} æ¡")
            print(f"åˆ›ä½œè€…æ•°æ®: {len(self.creators_data)} æ¡")
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬æ•°æ® - ä¿ç•™æ›´å¤šæœ‰ç”¨ä¿¡æ¯"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text)
        
        # ç§»é™¤URLé“¾æ¥
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ç§»é™¤é‚®ç®±
        text = re.sub(r'\S+@\S+', '', text)
        
        # ç§»é™¤@ç”¨æˆ·å
        text = re.sub(r'@[^\s]+', '', text)
        
        # ç§»é™¤#è¯é¢˜#
        text = re.sub(r'#[^#]+#', '', text)
        
        # ä¿ç•™æœ‰æ„ä¹‰çš„æ•°å­—ç»„åˆï¼ˆå¹´ä»½ã€é‡‘é¢ç­‰ï¼‰
        # åªç§»é™¤å•ç‹¬çš„çº¯æ•°å­—ï¼Œä¿ç•™ä¸æ–‡å­—ç»„åˆçš„æ•°å­—
        text = re.sub(r'\b\d{1,3}\b(?!\d)', '', text)  # åªç§»é™¤1-3ä½çš„ç‹¬ç«‹æ•°å­—
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def is_meaningful_word(self, word):
        """åˆ¤æ–­è¯è¯­æ˜¯å¦æœ‰æ„ä¹‰ - æ”¾å®½æ¡ä»¶"""
        # é•¿åº¦è¿‡æ»¤ - æ”¾å®½åˆ°1ä¸ªå­—ç¬¦ä¹Ÿå¯ä»¥ï¼ˆå¦‚"ç¾"ã€"ä¸­"ç­‰ï¼‰
        if len(word) < 1:
            return False
        
        # å¯¹äºå•å­—ç¬¦ï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„
        if len(word) == 1:
            meaningful_single_chars = {
                'ä¸­', 'ç¾', 'æ—¥', 'éŸ©', 'æ¬§', 'ä¿„', 'å°', 'è‹±', 'æ³•', 'å¾·', 'æ¾³',
                'é’±', 'æˆ¿', 'è½¦', 'è‚¡', 'å€º', 'é‡‘', 'é“¶', 'æ²¹', 'æ°”', 'ç…¤', 'é“',
                'å·¥', 'å†œ', 'å•†', 'å­¦', 'åŒ»', 'æ³•', 'ç†', 'æ–‡', 'å²', 'å“²'
            }
            return word in meaningful_single_chars
        
        # åœç”¨è¯è¿‡æ»¤
        if word.lower() in self.stop_words:
            return False
        
        # çº¯æ•°å­—è¿‡æ»¤ï¼ˆä½†ä¿ç•™å¹´ä»½ç­‰ï¼‰
        if word.isdigit():
            # ä¿ç•™å¹´ä»½
            if len(word) == 4 and word.startswith(('19', '20')):
                return True
            # ä¿ç•™å¤§é¢æ•°å­—ï¼ˆå¯èƒ½æ˜¯é‡‘é¢ã€æ’­æ”¾é‡ç­‰ï¼‰
            if len(word) >= 4:
                return True
            return False
        
        # é‡å¤å­—ç¬¦è¿‡æ»¤ï¼ˆå¦‚ï¼šå“ˆå“ˆå“ˆã€å‘µå‘µå‘µï¼‰
        if len(set(word)) == 1 and len(word) > 2:
            return False
        
        # æ˜æ˜¾çš„ç½‘ç»œç”¨è¯­è¿‡æ»¤ï¼ˆå‡å°‘æ•°é‡ï¼‰
        obvious_internet_slang = {
            'hhh', 'hhhh', 'emmm', 'awsl', 'orz', 'qaq', 'tql', 'yyds',
            'ç»ç»å­', 'çˆ±äº†çˆ±äº†', 'å†²å†²å†²', 'æ€æ€æ€', 'å‘œå‘œå‘œ', 'å˜¤å˜¤å˜¤',
            'ç¬‘å“­', 'æ‚è„¸', 'ç‹—å¤´', 'æ»‘ç¨½'
        }
        if word.lower() in obvious_internet_slang:
            return False
        
        return True
    
    def extract_keywords_advanced(self, text_list, top_k=30):
        """é«˜çº§å…³é”®è¯æå– - å¤šç§æ–¹æ³•ç»„åˆ"""
        # å°† top_k å‚æ•°ä¼ é€’ç»™æ–¹æ³•æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ config.yaml
        top_k = self.config.get("analysis", {}).get("top_keywords", top_k)

        # æ¸…ç†å’Œåˆå¹¶æ–‡æœ¬
        cleaned_texts = []
        for text in text_list:
            if text:
                cleaned = self.clean_text(text)
                if cleaned:
                    cleaned_texts.append(cleaned)
        
        if not cleaned_texts:
            return []
        
        combined_text = ' '.join(cleaned_texts)
        
        # æ–¹æ³•1: TF-IDF (æƒé‡è¾ƒé«˜)
        tfidf_keywords = jieba.analyse.extract_tags(
            combined_text, 
            topK=top_k*2,
            withWeight=True,
            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'vn', 'an', 'v', 'a', 'nrt')
        )
        
        # æ–¹æ³•2: TextRank (æƒé‡ä¸­ç­‰)
        textrank_keywords = jieba.analyse.textrank(
            combined_text, 
            topK=top_k*2,
            withWeight=True,
            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'vn', 'an', 'v', 'a', 'nrt')
        )
        
        # æ–¹æ³•3: è¯é¢‘ç»Ÿè®¡ (æƒé‡è¾ƒä½)
        words = jieba.lcut(combined_text)
        word_freq = Counter([w for w in words if len(w) > 1 and self.is_meaningful_word(w)])
        freq_keywords = [(word, freq/len(words)) for word, freq in word_freq.most_common(top_k*2)]
        
        # åˆå¹¶ç»“æœå¹¶åŠ æƒ
        keyword_scores = {}
        
        # TF-IDFæƒé‡ * 0.5
        for word, weight in tfidf_keywords:
            if self.is_meaningful_word(word):
                keyword_scores[word] = keyword_scores.get(word, 0) + weight * 0.5
        
        # TextRankæƒé‡ * 0.3
        for word, weight in textrank_keywords:
            if self.is_meaningful_word(word):
                keyword_scores[word] = keyword_scores.get(word, 0) + weight * 0.3
        
        # è¯é¢‘æƒé‡ * 0.2
        for word, weight in freq_keywords:
            if self.is_meaningful_word(word):
                keyword_scores[word] = keyword_scores.get(word, 0) + weight * 0.2
        
        # æ’åºå¹¶è¿”å›
        sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_keywords[:top_k]
    
    def extract_keywords(self, text_list, top_k=20, method='tfidf'):
        """æå–å…³é”®è¯ - å…¼å®¹åŸæ¥å£"""
        # å°† top_k å‚æ•°ä¼ é€’ç»™æ–¹æ³•æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ config.yaml
        top_k = self.config.get("analysis", {}).get("top_keywords", top_k)

        if method == 'advanced':
            return self.extract_keywords_advanced(text_list, top_k)
        
        # æ¸…ç†å’Œåˆå¹¶æ–‡æœ¬
        cleaned_texts = []
        for text in text_list:
            if text:
                cleaned = self.clean_text(text)
                if cleaned:
                    cleaned_texts.append(cleaned)
        
        if not cleaned_texts:
            return []
        
        combined_text = ' '.join(cleaned_texts)
        
        if method == 'tfidf':
            # ä½¿ç”¨TF-IDFæ–¹æ³• - æ”¾å®½è¯æ€§é™åˆ¶
            keywords = jieba.analyse.extract_tags(
                combined_text, 
                topK=top_k*3,  # æå–æ›´å¤šï¼Œç„¶åè¿‡æ»¤
                withWeight=True,
                allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'vn', 'an', 'v', 'a', 'nrt', 'ad', 'vd')
            )
        else:
            # ä½¿ç”¨TextRankæ–¹æ³•
            keywords = jieba.analyse.textrank(
                combined_text, 
                topK=top_k*3, 
                withWeight=True,
                allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'vn', 'an', 'v', 'a', 'nrt', 'ad', 'vd')
            )
        
        # è¿‡æ»¤æ— æ„ä¹‰è¯æ±‡
        meaningful_keywords = []
        for word, weight in keywords:
            if self.is_meaningful_word(word):
                meaningful_keywords.append((word, weight))
        
        # è¿”å›å‰top_kä¸ªæœ‰æ„ä¹‰çš„å…³é”®è¯
        return meaningful_keywords[:top_k]
    
    def sentiment_analysis(self, text):
        """æƒ…ç»ªåˆ†æ"""
        if not text or pd.isna(text):
            return 0.5, "ä¸­æ€§"
        try:
            s = SnowNLP(str(text))
            sentiment_score = s.sentiments
            # ä½¿ç”¨ config.yaml çš„é˜ˆå€¼
            pos_thres = self.config.get("analysis", {}).get("positive_threshold", 0.6)
            neg_thres = self.config.get("analysis", {}).get("negative_threshold", 0.4)
            if sentiment_score > pos_thres:
                sentiment_label = "ç§¯æ"
            elif sentiment_score < neg_thres:
                sentiment_label = "æ¶ˆæ"
            else:
                sentiment_label = "ä¸­æ€§"
            return sentiment_score, sentiment_label
        except:
            return 0.5, "ä¸­æ€§"
    
    def analyze_comments(self):
        """åˆ†æè¯„è®ºæ•°æ®"""
        print("\n=== è¯„è®ºæ–‡æœ¬åˆ†æ ===")
        if not self.comments_data:
            print("âŒ æ²¡æœ‰è¯„è®ºæ•°æ®")
            return None

        df_comments = pd.DataFrame(self.comments_data)
        total_comments = len(df_comments)
        valid_comments = df_comments['content'].notna().sum()
        print(f"æ€»è¯„è®ºæ•°: {total_comments}")
        print(f"æœ‰æ•ˆè¯„è®ºæ•°: {valid_comments}")

        df_comments['content_length'] = df_comments['content'].astype(str).str.len()
        print(f"å¹³å‡è¯„è®ºé•¿åº¦: {df_comments['content_length'].mean():.2f} å­—ç¬¦")
        print(f"æœ€é•¿è¯„è®º: {df_comments['content_length'].max()} å­—ç¬¦")
        print(f"æœ€çŸ­è¯„è®º: {df_comments['content_length'].min()} å­—ç¬¦")

        # === ä½¿ç”¨é…ç½®å‚æ•° ===
        sample_size = self.config.get("analysis", {}).get("comment_sample_size", 5000)
        top_k = self.config.get("analysis", {}).get("top_keywords", 20)

        # æƒ…ç»ªåˆ†æ
        print("\n--- è¯„è®ºæƒ…ç»ªåˆ†æ ---")
        sentiments = []
        sentiment_labels = []
        sample_size = min(sample_size, len(df_comments))
        sample_comments = df_comments.sample(n=sample_size)['content'].tolist()
        for comment in sample_comments:
            score, label = self.sentiment_analysis(comment)
            sentiments.append(score)
            sentiment_labels.append(label)
        sentiment_counts = Counter(sentiment_labels)
        print(f"ç§¯æè¯„è®º: {sentiment_counts['ç§¯æ']} ({sentiment_counts['ç§¯æ']/len(sentiment_labels)*100:.1f}%)")
        print(f"ä¸­æ€§è¯„è®º: {sentiment_counts['ä¸­æ€§']} ({sentiment_counts['ä¸­æ€§']/len(sentiment_labels)*100:.1f}%)")
        print(f"æ¶ˆæè¯„è®º: {sentiment_counts['æ¶ˆæ']} ({sentiment_counts['æ¶ˆæ']/len(sentiment_labels)*100:.1f}%)")
        print(f"å¹³å‡æƒ…ç»ªå¾—åˆ†: {np.mean(sentiments):.3f}")

        # å…³é”®è¯æå–
        print("\n--- è¯„è®ºå…³é”®è¯åˆ†æ ---")
        comment_texts = [str(comment) for comment in df_comments['content'].dropna()]
        print("ğŸ” ä½¿ç”¨é«˜çº§ç»„åˆæ–¹æ³•æå–å…³é”®è¯:")
        advanced_keywords = self.extract_keywords_advanced(comment_texts, top_k=top_k)
        for i, (word, weight) in enumerate(advanced_keywords[:15], 1):
            print(f"{i:2d}. {word}: {weight:.4f}")

        print("\nğŸ” ä½¿ç”¨TF-IDFæ–¹æ³•æå–å…³é”®è¯:")
        tfidf_keywords = self.extract_keywords(comment_texts, top_k=top_k, method='tfidf')
        for i, (word, weight) in enumerate(tfidf_keywords[:15], 1):
            print(f"{i:2d}. {word}: {weight:.4f}")

        print("\nğŸ” ä½¿ç”¨TextRankæ–¹æ³•æå–å…³é”®è¯:")
        textrank_keywords = self.extract_keywords(comment_texts, top_k=top_k, method='textrank')
        for i, (word, weight) in enumerate(textrank_keywords[:15], 1):
            print(f"{i:2d}. {word}: {weight:.4f}")

        like_counts = pd.to_numeric(df_comments['like_count'], errors='coerce').fillna(0)
        print(f"\n--- ç‚¹èµæ•°ç»Ÿè®¡ ---")
        print(f"å¹³å‡ç‚¹èµæ•°: {like_counts.mean():.2f}")
        print(f"æœ€é«˜ç‚¹èµæ•°: {like_counts.max()}")
        print(f"ç‚¹èµæ•°ä¸­ä½æ•°: {like_counts.median()}")

        return {
            'sentiment_distribution': dict(sentiment_counts),
            'sentiment_scores': sentiments,
            'advanced_keywords': advanced_keywords,
            'tfidf_keywords': tfidf_keywords,
            'textrank_keywords': textrank_keywords,
            'keywords': advanced_keywords,
            'basic_stats': {
                'total': int(total_comments),
                'valid': int(valid_comments),
                'avg_length': float(df_comments['content_length'].mean()),
                'max_length': int(df_comments['content_length'].max()),
                'min_length': int(df_comments['content_length'].min()),
                'avg_likes': float(like_counts.mean()),
                'max_likes': int(like_counts.max()),
                'median_likes': float(like_counts.median())
            }
        }
    
    def analyze_video_content(self):
        """åˆ†æè§†é¢‘å†…å®¹"""
        print("\n=== è§†é¢‘å†…å®¹åˆ†æ ===")
        
        if not self.contents_data:
            print("âŒ æ²¡æœ‰è§†é¢‘å†…å®¹æ•°æ®")
            return None
        
        df_contents = pd.DataFrame(self.contents_data)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"æ€»è§†é¢‘æ•°: {len(df_contents)}")
        
        # æ’­æ”¾é‡ç»Ÿè®¡
        play_counts = pd.to_numeric(df_contents['video_play_count'], errors='coerce').fillna(0)
        print(f"å¹³å‡æ’­æ”¾é‡: {play_counts.mean():.0f}")
        print(f"æœ€é«˜æ’­æ”¾é‡: {play_counts.max():.0f}")
        print(f"æ’­æ”¾é‡ä¸­ä½æ•°: {play_counts.median():.0f}")
        
        # æ ‡é¢˜åˆ†æ
        print("\n--- è§†é¢‘æ ‡é¢˜åˆ†æ ---")
        titles = df_contents['title'].dropna().tolist()
        title_keywords = self.extract_keywords_advanced(titles, top_k=20)
        
        print("æ ‡é¢˜çƒ­é—¨å…³é”®è¯:")
        for i, (word, weight) in enumerate(title_keywords[:15], 1):
            print(f"{i:2d}. {word}: {weight:.4f}")
        
        # æè¿°åˆ†æ
        print("\n--- è§†é¢‘æè¿°åˆ†æ ---")
        descriptions = df_contents['desc'].dropna().tolist()
        desc_keywords = self.extract_keywords_advanced(descriptions, top_k=20)
        
        print("æè¿°çƒ­é—¨å…³é”®è¯:")
        for i, (word, weight) in enumerate(desc_keywords[:15], 1):
            print(f"{i:2d}. {word}: {weight:.4f}")
        
        # æ ‡é¢˜æƒ…ç»ªåˆ†æ
        print("\n--- æ ‡é¢˜æƒ…ç»ªåˆ†æ ---")
        title_sentiments = []
        title_sentiment_labels = []
        
        for title in titles:
            score, label = self.sentiment_analysis(title)
            title_sentiments.append(score)
            title_sentiment_labels.append(label)
        
        title_sentiment_counts = Counter(title_sentiment_labels)
        print(f"ç§¯ææ ‡é¢˜: {title_sentiment_counts['ç§¯æ']} ({title_sentiment_counts['ç§¯æ']/len(title_sentiment_labels)*100:.1f}%)")
        print(f"ä¸­æ€§æ ‡é¢˜: {title_sentiment_counts['ä¸­æ€§']} ({title_sentiment_counts['ä¸­æ€§']/len(title_sentiment_labels)*100:.1f}%)")
        print(f"æ¶ˆææ ‡é¢˜: {title_sentiment_counts['æ¶ˆæ']} ({title_sentiment_counts['æ¶ˆæ']/len(title_sentiment_labels)*100:.1f}%)")
        
        return {
            'title_keywords': title_keywords,
            'desc_keywords': desc_keywords,
            'title_sentiment': dict(title_sentiment_counts),
            'title_sentiment_scores': title_sentiments,
            'video_stats': {
                'total_videos': int(len(df_contents)),
                'avg_play_count': float(play_counts.mean()),
                'max_play_count': int(play_counts.max()),
                'median_play_count': float(play_counts.median())
            }
        }
    
    def analyze_creators(self):
        """åˆ†æåˆ›ä½œè€…æ•°æ®"""
        print("\n=== åˆ›ä½œè€…åˆ†æ ===")
        
        if not self.creators_data:
            print("âŒ æ²¡æœ‰åˆ›ä½œè€…æ•°æ®")
            return None
        
        df_creators = pd.DataFrame(self.creators_data)
        
        # æ€§åˆ«åˆ†å¸ƒ
        gender_dist = df_creators['sex'].value_counts()
        print("--- åˆ›ä½œè€…æ€§åˆ«åˆ†å¸ƒ ---")
        for gender, count in gender_dist.items():
            print(f"{gender}: {count} ({count/len(df_creators)*100:.1f}%)")
        
        # ç²‰ä¸æ•°åˆ†æ
        fan_counts = pd.to_numeric(df_creators['total_fans'], errors='coerce').fillna(0)
        print(f"\n--- ç²‰ä¸æ•°ç»Ÿè®¡ ---")
        print(f"å¹³å‡ç²‰ä¸æ•°: {fan_counts.mean():.0f}")
        print(f"æœ€é«˜ç²‰ä¸æ•°: {fan_counts.max():.0f}")
        print(f"ç²‰ä¸æ•°ä¸­ä½æ•°: {fan_counts.median():.0f}")
        
        # ä¸ªæ€§ç­¾åå…³é”®è¯åˆ†æ
        print("\n--- ä¸ªæ€§ç­¾åå…³é”®è¯ ---")
        signs = df_creators['sign'].dropna().tolist()
        sign_keywords = self.extract_keywords_advanced(signs, top_k=15)
        
        for i, (word, weight) in enumerate(sign_keywords[:15], 1):
            print(f"{i:2d}. {word}: {weight:.4f}")
        
        return {
            'gender_distribution': dict(gender_dist),
            'fan_stats': {
                'avg_fans': float(fan_counts.mean()),
                'max_fans': int(fan_counts.max()),
                'median_fans': float(fan_counts.median())
            },
            'sign_keywords': sign_keywords
        }
    
    def generate_wordcloud(self, keywords, title="è¯äº‘å›¾", save_path=None):
        """ç”Ÿæˆè¯äº‘å›¾"""
        if not keywords:
            print("âŒ æ²¡æœ‰å…³é”®è¯æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè¯äº‘")
            return
        word_freq = {word: weight for word, weight in keywords}
        # è¯»å–è¯äº‘é…ç½®
        wc_cfg = self.config.get("analysis", {}).get("wordcloud", {})
        width = wc_cfg.get("width", 1000)
        height = wc_cfg.get("height", 500)
        max_words = wc_cfg.get("max_words", 150)
        background_color = wc_cfg.get("background_color", "white")
        colormap = wc_cfg.get("colormap", "viridis")
        wordcloud = WordCloud(
            font_path='C:/Windows/Fonts/simhei.ttf',
            width=width,
            height=height,
            background_color=background_color,
            max_words=max_words,
            colormap=colormap,
            prefer_horizontal=0.7
        ).generate_from_frequencies(word_freq)
        plt.figure(figsize=(15, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(title, fontsize=18)
        plt.axis('off')
        plt.tight_layout()
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ è¯äº‘å›¾å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()
    
    def create_visualizations(self, comment_analysis, content_analysis, creator_analysis):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        fig, axes = plt.subplots(2, 3, figsize=(20, 14))
        
        # 1. è¯„è®ºæƒ…ç»ªåˆ†å¸ƒ
        if comment_analysis and 'sentiment_distribution' in comment_analysis:
            sentiment_data = comment_analysis['sentiment_distribution']
            colors = ['#ff9999', '#66b3ff', '#99ff99']
            axes[0, 0].pie(sentiment_data.values(), labels=sentiment_data.keys(), 
                          autopct='%1.1f%%', colors=colors)
            axes[0, 0].set_title('è¯„è®ºæƒ…ç»ªåˆ†å¸ƒ', fontsize=14)
        
        # 2. è¯„è®ºé•¿åº¦åˆ†å¸ƒ
        if self.comments_data:
            df_comments = pd.DataFrame(self.comments_data)
            df_comments['content_length'] = df_comments['content'].astype(str).str.len()
            axes[0, 1].hist(df_comments['content_length'], bins=50, alpha=0.7, color='skyblue')
            axes[0, 1].set_title('è¯„è®ºé•¿åº¦åˆ†å¸ƒ', fontsize=14)
            axes[0, 1].set_xlabel('è¯„è®ºé•¿åº¦ (å­—ç¬¦)')
            axes[0, 1].set_ylabel('é¢‘æ¬¡')
        
        # 3. åˆ›ä½œè€…æ€§åˆ«åˆ†å¸ƒ
        if creator_analysis and 'gender_distribution' in creator_analysis:
            gender_data = creator_analysis['gender_distribution']
            axes[0, 2].bar(gender_data.keys(), gender_data.values(), color=['pink', 'lightblue', 'lightgreen'])
            axes[0, 2].set_title('åˆ›ä½œè€…æ€§åˆ«åˆ†å¸ƒ', fontsize=14)
            axes[0, 2].set_ylabel('äººæ•°')
        
        # 4. è§†é¢‘æ’­æ”¾é‡åˆ†å¸ƒ
        if self.contents_data:
            df_contents = pd.DataFrame(self.contents_data)
            play_counts = pd.to_numeric(df_contents['video_play_count'], errors='coerce').dropna()
            axes[1, 0].hist(play_counts, bins=30, alpha=0.7, color='lightgreen')
            axes[1, 0].set_title('è§†é¢‘æ’­æ”¾é‡åˆ†å¸ƒ', fontsize=14)
            axes[1, 0].set_xlabel('æ’­æ”¾é‡')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
        
        # 5. çƒ­é—¨å…³é”®è¯
        if comment_analysis and 'keywords' in comment_analysis:
            keywords = comment_analysis['keywords'][:12]  # æ˜¾ç¤ºæ›´å¤šå…³é”®è¯
            words = [word for word, _ in keywords]
            weights = [weight for _, weight in keywords]
            
            y_pos = range(len(words))
            axes[1, 1].barh(y_pos, weights, color='orange')
            axes[1, 1].set_yticks(y_pos)
            axes[1, 1].set_yticklabels(words)
            axes[1, 1].set_title('è¯„è®ºçƒ­é—¨å…³é”®è¯', fontsize=14)
            axes[1, 1].set_xlabel('æƒé‡')
        
        # 6. æ ‡é¢˜æƒ…ç»ªåˆ†å¸ƒ
        if content_analysis and 'title_sentiment' in content_analysis:
            title_sentiment = content_analysis['title_sentiment']
            axes[1, 2].pie(title_sentiment.values(), labels=title_sentiment.keys(), 
                          autopct='%1.1f%%', colors=['#ff9999', '#66b3ff', '#99ff99'])
            axes[1, 2].set_title('è§†é¢‘æ ‡é¢˜æƒ…ç»ªåˆ†å¸ƒ', fontsize=14)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        save_path = 'results/analysis_charts.png'
        os.makedirs('results', exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def convert_to_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self.convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_to_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return [self.convert_to_serializable(item) for item in obj]
        else:
            return obj
    
    def comprehensive_analysis(self):
        """ç»¼åˆåˆ†æ"""
        print("ğŸš€ å¼€å§‹ç»¼åˆæ–‡æœ¬åˆ†æ...")
        
        # åŠ è½½æ•°æ®
        self.load_data()
        
        # åˆ†æè¯„è®º
        comment_analysis = self.analyze_comments()
        
        # åˆ†æè§†é¢‘å†…å®¹
        content_analysis = self.analyze_video_content()
        
        # åˆ†æåˆ›ä½œè€…
        creator_analysis = self.analyze_creators()
        
        # ç”Ÿæˆå¯è§†åŒ–
        print("\n=== ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        self.create_visualizations(comment_analysis, content_analysis, creator_analysis)
        
        # ç”Ÿæˆè¯äº‘å›¾
        if comment_analysis and 'keywords' in comment_analysis:
            print("\n=== ç”Ÿæˆè¯„è®ºè¯äº‘å›¾ ===")
            self.generate_wordcloud(comment_analysis['keywords'], 
                                  "è¯„è®ºå…³é”®è¯è¯äº‘", 
                                  "results/comment_wordcloud.png")
        
        if content_analysis and 'title_keywords' in content_analysis:
            print("\n=== ç”Ÿæˆæ ‡é¢˜è¯äº‘å›¾ ===")
            self.generate_wordcloud(content_analysis['title_keywords'], 
                                  "è§†é¢‘æ ‡é¢˜å…³é”®è¯è¯äº‘",
                                  "results/title_wordcloud.png")
        
        print("\nâœ… åˆ†æå®Œæˆï¼")
        
        return {
            'comment_analysis': comment_analysis,
            'content_analysis': content_analysis,
            'creator_analysis': creator_analysis
        }

def main():
    """ä¸»å‡½æ•°"""
    analyzer = BilibiliTextAnalyzer()
    results = analyzer.comprehensive_analysis()
    
    # ä¿å­˜åˆ†æç»“æœ
    try:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = analyzer.convert_to_serializable(results)
        
        # ä¿å­˜æ›´å¤šå…³é”®è¯
        if 'comment_analysis' in serializable_results and serializable_results['comment_analysis']:
            if 'keywords' in serializable_results['comment_analysis']:
                serializable_results['comment_analysis']['keywords'] = serializable_results['comment_analysis']['keywords'][:30]
            if 'advanced_keywords' in serializable_results['comment_analysis']:
                serializable_results['comment_analysis']['advanced_keywords'] = serializable_results['comment_analysis']['advanced_keywords'][:30]
            if 'tfidf_keywords' in serializable_results['comment_analysis']:
                serializable_results['comment_analysis']['tfidf_keywords'] = serializable_results['comment_analysis']['tfidf_keywords'][:30]
            if 'textrank_keywords' in serializable_results['comment_analysis']:
                serializable_results['comment_analysis']['textrank_keywords'] = serializable_results['comment_analysis']['textrank_keywords'][:30]
        
        if 'content_analysis' in serializable_results and serializable_results['content_analysis']:
            if 'title_keywords' in serializable_results['content_analysis']:
                serializable_results['content_analysis']['title_keywords'] = serializable_results['content_analysis']['title_keywords'][:30]
            if 'desc_keywords' in serializable_results['content_analysis']:
                serializable_results['content_analysis']['desc_keywords'] = serializable_results['content_analysis']['desc_keywords'][:30]
        
        if 'creator_analysis' in serializable_results and serializable_results['creator_analysis']:
            if 'sign_keywords' in serializable_results['creator_analysis']:
                serializable_results['creator_analysis']['sign_keywords'] = serializable_results['creator_analysis']['sign_keywords'][:30]
        
        # æ·»åŠ åˆ†ææ—¶é—´æˆ³
        serializable_results['analysis_timestamp'] = datetime.now().isoformat()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs('results', exist_ok=True)
        with open('results/analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        print("ğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ° results/analysis_results.json")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        generate_analysis_report(serializable_results)
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")

def generate_analysis_report(results):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    try:
        report_content = f"""# Bç«™æ•°æ®åˆ†ææŠ¥å‘Š

## åˆ†ææ—¶é—´
{results.get('analysis_timestamp', 'æœªçŸ¥')}

## æ•°æ®æ¦‚è§ˆ
"""
        
        # è¯„è®ºåˆ†æéƒ¨åˆ†
        if 'comment_analysis' in results and results['comment_analysis']:
            comment_stats = results['comment_analysis']['basic_stats']
            sentiment_dist = results['comment_analysis']['sentiment_distribution']
            
            report_content += f"""
### è¯„è®ºæ•°æ®åˆ†æ
- **æ€»è¯„è®ºæ•°**: {comment_stats['total']:,} æ¡
- **æœ‰æ•ˆè¯„è®ºæ•°**: {comment_stats['valid']:,} æ¡
- **å¹³å‡è¯„è®ºé•¿åº¦**: {comment_stats['avg_length']:.2f} å­—ç¬¦
- **å¹³å‡ç‚¹èµæ•°**: {comment_stats['avg_likes']:.2f}

#### æƒ…ç»ªåˆ†å¸ƒ
- ç§¯æè¯„è®º: {sentiment_dist.get('ç§¯æ', 0)} æ¡ ({sentiment_dist.get('ç§¯æ', 0)/(sentiment_dist.get('ç§¯æ', 0)+sentiment_dist.get('ä¸­æ€§', 0)+sentiment_dist.get('æ¶ˆæ', 0))*100:.1f}%)
- ä¸­æ€§è¯„è®º: {sentiment_dist.get('ä¸­æ€§', 0)} æ¡ ({sentiment_dist.get('ä¸­æ€§', 0)/(sentiment_dist.get('ç§¯æ', 0)+sentiment_dist.get('ä¸­æ€§', 0)+sentiment_dist.get('æ¶ˆæ', 0))*100:.1f}%)
- æ¶ˆæè¯„è®º: {sentiment_dist.get('æ¶ˆæ', 0)} æ¡ ({sentiment_dist.get('æ¶ˆæ', 0)/(sentiment_dist.get('ç§¯æ', 0)+sentiment_dist.get('ä¸­æ€§', 0)+sentiment_dist.get('æ¶ˆæ', 0))*100:.1f}%)

#### çƒ­é—¨å…³é”®è¯ (é«˜çº§ç»„åˆç®—æ³•)
"""
            if 'advanced_keywords' in results['comment_analysis']:
                keywords = results['comment_analysis']['advanced_keywords'][:20]
                for i, (word, weight) in enumerate(keywords, 1):
                    report_content += f"{i}. {word} (æƒé‡: {weight:.4f})\n"
            
            if 'tfidf_keywords' in results['comment_analysis']:
                report_content += "\n#### çƒ­é—¨å…³é”®è¯ (TF-IDF)\n"
                keywords = results['comment_analysis']['tfidf_keywords'][:15]
                for i, (word, weight) in enumerate(keywords, 1):
                    report_content += f"{i}. {word} (æƒé‡: {weight:.4f})\n"
            
            if 'textrank_keywords' in results['comment_analysis']:
                report_content += "\n#### çƒ­é—¨å…³é”®è¯ (TextRank)\n"
                keywords = results['comment_analysis']['textrank_keywords'][:15]
                for i, (word, weight) in enumerate(keywords, 1):
                    report_content += f"{i}. {word} (æƒé‡: {weight:.4f})\n"
        
        # è§†é¢‘å†…å®¹åˆ†æéƒ¨åˆ†
        if 'content_analysis' in results and results['content_analysis']:
            video_stats = results['content_analysis']['video_stats']
            title_sentiment = results['content_analysis']['title_sentiment']
            
            report_content += f"""
### è§†é¢‘å†…å®¹åˆ†æ
- **æ€»è§†é¢‘æ•°**: {video_stats['total_videos']} ä¸ª
- **å¹³å‡æ’­æ”¾é‡**: {video_stats['avg_play_count']:,.0f}
- **æœ€é«˜æ’­æ”¾é‡**: {video_stats['max_play_count']:,}
- **æ’­æ”¾é‡ä¸­ä½æ•°**: {video_stats['median_play_count']:,.0f}

#### æ ‡é¢˜æƒ…ç»ªåˆ†å¸ƒ
- ç§¯ææ ‡é¢˜: {title_sentiment.get('ç§¯æ', 0)} ä¸ª
- ä¸­æ€§æ ‡é¢˜: {title_sentiment.get('ä¸­æ€§', 0)} ä¸ª
- æ¶ˆææ ‡é¢˜: {title_sentiment.get('æ¶ˆæ', 0)} ä¸ª

#### è§†é¢‘æ ‡é¢˜çƒ­é—¨å…³é”®è¯
"""
            if 'title_keywords' in results['content_analysis']:
                keywords = results['content_analysis']['title_keywords'][:15]
                for i, (word, weight) in enumerate(keywords, 1):
                    report_content += f"{i}. {word} (æƒé‡: {weight:.4f})\n"
        
        # åˆ›ä½œè€…åˆ†æéƒ¨åˆ†  
        if 'creator_analysis' in results and results['creator_analysis']:
            gender_dist = results['creator_analysis']['gender_distribution']
            fan_stats = results['creator_analysis']['fan_stats']
            
            report_content += f"""
### åˆ›ä½œè€…åˆ†æ
#### æ€§åˆ«åˆ†å¸ƒ
"""
            for gender, count in gender_dist.items():
                report_content += f"- {gender}: {count} äºº\n"
            
            report_content += f"""
#### ç²‰ä¸æ•°ç»Ÿè®¡
- **å¹³å‡ç²‰ä¸æ•°**: {fan_stats['avg_fans']:,.0f}
- **æœ€é«˜ç²‰ä¸æ•°**: {fan_stats['max_fans']:,}
- **ç²‰ä¸æ•°ä¸­ä½æ•°**: {fan_stats['median_fans']:,.0f}

#### ä¸ªæ€§ç­¾åçƒ­é—¨å…³é”®è¯
"""
            if 'sign_keywords' in results['creator_analysis']:
                keywords = results['creator_analysis']['sign_keywords'][:10]
                for i, (word, weight) in enumerate(keywords, 1):
                    report_content += f"{i}. {word} (æƒé‡: {weight:.4f})\n"
        
        # ä¿å­˜æŠ¥å‘Š
        with open('results/analysis_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        print("ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° results/analysis_report.md")
        
    except Exception as e:
        print(f"âš ï¸ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main()